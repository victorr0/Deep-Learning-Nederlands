{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning in het Nederlands\n",
    "=============\n",
    "\n",
    "Tensorflow in Dutch\n",
    "------------\n",
    "In de reeks `Deep Learning in het Nederlands - Tensorflow in Dutch` maak je kennis met de methodes en technieken voor Deep Learning met Tensorflow. Experimenteer, duik in het diepe of leer bij met de 6 opdrachten. De volledige repository staat op mijn [Github](https://github.com/victorr0/Deep-Learning-Nederlands). \n",
    "\n",
    "TensorFlow is een open source software bibliotheek voor Machine Learning waarmee je neurale netwerken kunt bouwen en trainen om patronen en correlaties te detecteren en te ontcijferen: [Tensorflow](https://github.com/tensorflow/tensorflow.git). \n",
    "\n",
    "Deze opdrachten volgen het leertraject van [Udacity](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity) en zijn in het Engels beschikbaar in de standaard TensorFlow repository. Ik heb ze naar het Nederlands vertaald en in een repository geplaatst. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opdracht 2\n",
    "------------\n",
    "\n",
    "In de vorige opdracht 1_notmnist.ipynb hebben we een pickle-bestand gemaakt met de geformatteerde datasets voor de training, ontwikkeling en het testen met de [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset.\n",
    "\n",
    "Het doel van deze opdracht is om stapsgewijs modellen te trainen die dieper en nauwkeuriger worden met behulp van TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# Dit zijn alle modules die nodig zijn voor dit notebook. \n",
    "# Zorg dat ze allemaal geimporteerd kunnen worden\n",
    "# voordat je verder gaat.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "Eerst laden we opnieuw de data uit `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # Hint om geheugen vrij te maken.\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Nu formatteren we de data zodat die beter past bij de modellen die we gaan trainen:\n",
    "- data als een flat matrix,\n",
    "- labels als float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map van 0 naar [1.0, 0.0, 0.0 ...], 1 naar [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We gaan eerst een multinomiale logistieke regressie trainen met behulp van simple gradient descent.\n",
    "\n",
    "TensorFlow werkt als volgt:\n",
    "* Eerst beschrijf je de berekening die uitgevoerd moet worden: hoe de inputs, de variabelen en de operations eruit moeten zien. Die worden gecreëerd als nodes (knooppunten) over een berekeningsgrafiek. Deze beschrijving wordt omvat in het onderstaande blok:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Vervolgens kun je de bewerkingen op deze grafiek zo vaak uitvoeren als je wilt door `session.run()` aan te roepen. Je krijgt dan de output uit de grafiek terug. Deze runtime-operatie wordt omvat in het onderstaande blok:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Nu laden we alle data in Tensorflow en bouwen de berekeningsgrafiek op die overeenkomt met onze training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# Met gradiënte afdaling (gradient descent) training is deze berekening computationeel erg zwaar.\n",
    "# Daarom verdelen we de training data, met een snellere turnaround (omkeertijd) als resultaat.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Laad de training, validation and test data naar constanten\n",
    "  # gekoppeld aan de grafiek.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variabelen.\n",
    "  # Dit zijn de parameters die we gaan trainen. De gewicht\n",
    "  # matrix (weight) wordt geïnitialiseerd met behulp van willekeurige waarden \n",
    "  # volgens een (verkorte) normale verdeling. De biases\n",
    "  # worden geïnitialiseerd naar nul.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training berekeningen.\n",
    "  # We vermenigvuldigen de invoer met de gewichtsmatrix en voegen de bias toe.\n",
    "  # We berekenen de softmax en cross-entropy (dit is een operation in TensorFlow, \n",
    "  # omdat het heel vaak voorkomt, en het kan worden geoptimaliseerd).\n",
    "  # We nemen het gemiddelde van deze cross-entropie over alle trainingsvoorbeelden: \n",
    "  # dat is ons verlies (loss).\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimalisator (Optimizer).\n",
    "  # We gaan het minimum van dit verlies vinden door gebruik te maken van gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Voorspellingen  (predictions) voor de training, validation en test data.\n",
    "  # Deze zijn niet onderdeel van training, maar staan hier alleen zodat we \n",
    "  # nauwkeurigheidscijfers kunnen rapporteren terwijl we trainen.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Laten we deze berekening uitvoeren en itereren (herhalen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 : 17.2939\n",
      "Training accuracy: 10.8%\n",
      "Validation accuracy: 13.8%\n",
      "Loss at step 100 : 2.26903\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 71.6%\n",
      "Loss at step 200 : 1.84895\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 73.9%\n",
      "Loss at step 300 : 1.60701\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 400 : 1.43912\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 500 : 1.31349\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 600 : 1.21501\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.4%\n",
      "Loss at step 700 : 1.13515\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 75.4%\n",
      "Loss at step 800 : 1.0687\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 75.6%\n",
      "Test accuracy: 82.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Dit is een eenmalige operatie die ervoor zorgt dat de parameters worden geïnitialiseerd \n",
    "  # zoals we in de grafiek beschrijven: willekeurige gewichten (weights) voor de matrix, nullen (zeros) \n",
    "  # voor de voorspellingen.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Geïnitialiseerd')\n",
    "  for step in range(num_steps):\n",
    "  # Voer de berekeningen uit. We vertellen .run () dat we de optimizer willen uitvoeren \n",
    "  # en de verlieswaarde en de trainingsvoorspellingen terug willen als numpy arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Verlies bij stap %d: %f' % (step, l))\n",
    "      print('Training nauwkeurigheid: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Het aanroepen van .eval() op valid_prediction is in principe hetzelfde als .run() aanroepen, \n",
    "      # maar alleen om die ene numpy array te krijgen. Merk op dat het al de grafische \n",
    "      # afhankelijkheden opnieuw berekent.\n",
    "      print('Validation nauwkeurigheid: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test nauwkeurigheid: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Laten we nu switchen naar stochastic gradient descent training, iets dat veel sneller werkt!\n",
    "\n",
    "De grafiek zal vergelijkbaar zijn, behalve dat in plaats van alle trainingsgegevens in een constante node te houden, creëren we een `Placeholder`-node, dat de daadwerkelijke gegevens zal invoeren bij elke aanroep van ` session.run() `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. Voor de trainings data gebruiken we een plaatshouder \n",
    "  # die tijdens de het uitvoeren wordt gevoed met een training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variabelen\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training berekeningen.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimalisator (Optimizer).\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Voorspellingen voor de training, validation en test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Dan is het nu tijd om uit te voeren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 16.8091\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 14.0%\n",
      "Minibatch loss at step 500 : 1.75256\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 1000 : 1.32283\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1500 : 0.944533\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 2000 : 1.03795\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 2500 : 1.10219\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 3000 : 0.758874\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.8%\n",
      "Test accuracy: 86.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Kies een compensatie (offset) voor de trainings data, die gerandomiseerd zijn.\n",
    "    # Opmerking: we kunnen betere randomisatie gebruiken over epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Genereer een minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Bereid een dictionary voor die de sessie aangeeft waar de minibatch heen moet.\n",
    "    # De key in de dictionary is de plaatshouder-node van de grafiek die moet worden ingevoerd, \n",
    "    # en de value is de numpy array die vervolgens in de grafiek wordt gevoerd.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch verlies bij stap %d: %f\" % (step, l))\n",
    "      print(\"Minibatch nauwkeurigheid: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation nauwkeurigheid: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test nauwkeurigheid: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Probleem/vraag\n",
    "-------\n",
    "\n",
    "Verander het logistic regression voorbeeld met SGD naar een 1-hidden layer (verborgen laag) neural network met  rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) en 1024 hidden nodes (knooppunten). Dit model moet je validatie- en test-nauwkeurigheid verbeteren.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
