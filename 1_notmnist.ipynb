{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "Deep Learning in het Nederlands\n",
    "=============\n",
    "\n",
    "Tensorflow in Dutch\n",
    "------------\n",
    "In de reeks `Deep Learning in het Nederlands - Tensorflow in Dutch` maak je kennis met de methodes en technieken voor Deep Learning met Tensorflow. Experimenteer, duik in het diepe of leer bij met de 6 opdrachten. De volledige repository staat op mijn [Github](https://github.com/victorr0/Deep-Learning-Nederlands). \n",
    "\n",
    "TensorFlow is een open source software bibliotheek voor Machine Learning waarmee je neurale netwerken kunt bouwen en trainen om patronen en correlaties te detecteren en te ontcijferen: [Tensorflow](https://github.com/tensorflow/tensorflow.git). \n",
    "\n",
    "Deze opdrachten volgen het leertraject van [Udacity](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity) en zijn in het Engels beschikbaar in de standaard TensorFlow repository. Ik heb ze naar het Nederlands vertaald en in een repository geplaatst. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opdracht 1\n",
    "------------\n",
    "\n",
    "Het doel van deze opdracht is het leren van eenvoudige data verzamel praktijken en vertrouwd raken met een aantal van de gegevens die we later zullen hergebruiken. Dit notebook maakt gebruik van de [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset die gebruikt wordt met python experimenten. \n",
    "\n",
    "Deze dataset lijkt op de klassieke [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, alleen bevat meer realistischere data zoals je die in het echt tegenkomt: de taak is zwaarder en de data is veel minder 'schoon' dan MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "# Dit zijn alle modules die nodig zijn voor dit notebook. \n",
    "# Zorg dat ze allemaal geimporteerd kunnen worden\n",
    "# voordat je verder gaat.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Configureer matplotlib backend om later in de opdracht inline in IPython te plotten.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNWGtZaXn-5j"
   },
   "source": [
    "Als eerste gaan we de dataset downloaden naar onze lokale machine. De gegevens bestaan uit characters (letters A t/m J) die in verschillende lettertypes worden weergegeven in afbeeldingen van 28 x 28 pixels.\n",
    "\n",
    "De labels zijn beperkt tot 'A' tot 'J' (10 klassen/classes). De trainingsset heeft ongeveer 500k en de testset 19000 gelabelde voorbeelden. Gezien de grootte van de set hoor je de modellen op elke computer redelijk snel te kunnen trainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified notMNIST_large.tar.gz\n",
      "Found and verified notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Verander deze variabele om de gegevens ergens anders op te slaan.\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Poging om te downloaden:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload voltooid!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Gevonden en geverifieerd', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Kon het volgende niet verifiÃ«ren: ' + dest_filename + '. Kun je het handmatig met een browser downloaden?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC3p0oEyF8QT"
   },
   "source": [
    "Pak de dataset in het .tar.gz bestand uit.\n",
    "Je hebt nu een aantal directories gelabeled van A tot en met J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186055,
     "status": "ok",
     "timestamp": 1444485672525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "H8CBE-WZ8nmj",
    "outputId": "ef6c790c-2513-4b09-962e-27c79390c762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']\n",
      "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # Verwijder .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # Je kunt dit overriden door het veranderen van force=False naar force = True.\n",
    "    print('%s Al aanwezig - Uitpakken overgeslagen voor %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Data uitpakken voor %s. Dit kan een tijdje duren. Even geduld aub.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Verwachtte %d mappen, 1 per class. Echter zijn er %d gevonden.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "---\n",
    "Probleem/vraag 1\n",
    "---------\n",
    "\n",
    "Laten we eens kijken naar de data om te checken hoe het er uitziet. Elk exemplar (voorbeeld) hoort een afbeelding te zijn van een letter A tot en met J in een ander lettertype. Laat een voorbeeld zien van de afbeeldingen die we zojuist hebben gedownload. \n",
    "\n",
    "Hint: Gebruik hiervoor de module IPython.display.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBdkjESPK8tw"
   },
   "source": [
    "Nu laden we de data in een praktischer formaat. Omdat afhankelijk van de computerinstellingen misschien niet alle data in het geheugen past, laden we elke class in een aparte dataset en slaan die onafhankelijk van elkaar op. Later gaan we ze weer samenvoegen tot 1 overzichtelijke dataset.\n",
    "\n",
    "We converteren de hele dataset naar een 3D-array (afbeelding-index, x, y) van floating point-waardes, genormaliseerd met een gemiddelde van ongeveer 0 en een standaardafwijking ~ 0,5. Dit maakt het training gemakkelijker. \n",
    "\n",
    "Sommige afbeeldingen zijn misschien niet leesbaar, in dat geval slaan we die over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large/A\n",
      "Could not read: notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file - it's ok, skipping.\n",
      "Could not read: notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file - it's ok, skipping.\n",
      "Could not read: notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12848\n",
      "Standard deviation: 0.425576\n",
      "notMNIST_large/B\n",
      "Could not read: notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00755947\n",
      "Standard deviation: 0.417272\n",
      "notMNIST_large/C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142321\n",
      "Standard deviation: 0.421305\n",
      "notMNIST_large/D\n",
      "Could not read: notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0574553\n",
      "Standard deviation: 0.434072\n",
      "notMNIST_large/E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0701406\n",
      "Standard deviation: 0.42882\n",
      "notMNIST_large/F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125914\n",
      "Standard deviation: 0.429645\n",
      "notMNIST_large/G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0947771\n",
      "Standard deviation: 0.421674\n",
      "notMNIST_large/H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0687667\n",
      "Standard deviation: 0.430344\n",
      "notMNIST_large/I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307405\n",
      "Standard deviation: 0.449686\n",
      "notMNIST_large/J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153479\n",
      "Standard deviation: 0.397169\n",
      "notMNIST_small/A\n",
      "Could not read: notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132588\n",
      "Standard deviation: 0.445923\n",
      "notMNIST_small/B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535619\n",
      "Standard deviation: 0.457054\n",
      "notMNIST_small/C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141489\n",
      "Standard deviation: 0.441056\n",
      "notMNIST_small/D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492094\n",
      "Standard deviation: 0.460477\n",
      "notMNIST_small/E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0598952\n",
      "Standard deviation: 0.456146\n",
      "notMNIST_small/F\n",
      "Could not read: notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118148\n",
      "Standard deviation: 0.451134\n",
      "notMNIST_small/G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.092519\n",
      "Standard deviation: 0.448468\n",
      "notMNIST_small/H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586729\n",
      "Standard deviation: 0.457387\n",
      "notMNIST_small/I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526481\n",
      "Standard deviation: 0.472657\n",
      "notMNIST_small/J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.15167\n",
      "Standard deviation: 0.449521\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel breedte en hoogte.\n",
    "pixel_depth = 255.0  # Aantal levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Onverwachtte afbeelding-grootte: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Kon het volgende niet lezen:', image_file, ':', e, '- geen probleem, slaan we over.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Veel minder afbeelding dan verwacht: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Volledige dataset tensor:', dataset.shape)\n",
    "  print('Mean (gemiddelde):', np.mean(dataset))\n",
    "  print('Standard deviation (standaard deviatie):', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # Je kunt dit overriden door het veranderen van force=False naar force = True.\n",
    "      print('%s bestaat al - pickling wordt overgeslagen.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Kon het volgende niet opslaan', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUdbskYE2d87"
   },
   "source": [
    "---\n",
    "Probleem/vraag 2\n",
    "---------\n",
    "\n",
    "Laten we checken of de data er nog steeds goed uitzien. Toon een voorbeeld van de afbeeldingen en labels uit de ndarray. \n",
    "\n",
    "Hint: Gebruik hiervoor de module matplotlib.pyplot.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYznx5jUwzoO"
   },
   "source": [
    "---\n",
    "Probleem/vraag 3\n",
    "---------\n",
    "Nog een check: De gegevens horen in evenwicht te zijn tussen de verschillende klassen. Verifieer dat.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LA7M7K22ynCt"
   },
   "source": [
    "Knip, plak of pas de trainingsgegevens aan indien nodig. Afhankelijk van uw computerinstellingen kan de data niet allemaal tegelijkertijd in het geheugen passen. Je kunt 'train_size' aanpassen indien nodig. \n",
    "\n",
    "De labels (A t/m J) worden opgeslagen in een aparte array van integer getallen 0 tot en met 9.\n",
    "\n",
    "Maak ook een validatie dataset voor hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411281,
     "status": "ok",
     "timestamp": 1444485897869,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "s3mWgZLpyuzq",
    "outputId": "8af66da6-902d-4719-bedc-7c9fb7ae7948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (200000, 28, 28) (200000,)\n",
      "Validation (10000, 28, 28) (10000,)\n",
      "Testing (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # Nu schudden we de letters zodat de validatie en trainingsset helemaal willekeurig/random zijn.\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Kon het volgende niet verwerken', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "Vervolgens maken we de dataset's volgorde willekeurig. Het is belangrijk om de labels goed te schudden voor de training en de testverdelingen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6WZ2l2tN2zOL"
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puDUTe6t6USl"
   },
   "source": [
    "---\n",
    "Probleem/vraag 4\n",
    "---------\n",
    "Overtuig jezelf ervan dat de data nog steeds goed is/klopt na het schudden.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIQJaJuwg5Hw"
   },
   "source": [
    "Tot slot slaan we de data op voor hergebruik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QiR_rETzem6C"
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Kon het volgende niet opslaan', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 413065,
     "status": "ok",
     "timestamp": 1444485899688,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "hQbLjrW_iT39",
    "outputId": "b440efc6-5ee1-4cbc-d02d-93db44ebd956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 718193801\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Gecomprimeerde pickle grootte:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gE_cRAQB33lk"
   },
   "source": [
    "---\n",
    "Probleem/vraag 5\n",
    "---------\n",
    "Door de samenstelling van deze dataset kunnen er veel overlappende metingen/samples zijn, inclusief trainingsgegevens die zowel in de validatie- als in de testset zitten! \n",
    "\n",
    "Overlap tussen training en test kan een vertekend beeld geven van de resultaten, vooral als je het model gebruik in een omgeving waar nooit overlap plaatsvindt, maar het eigenlijk goed als je verwacht dat de samples terugkomen bij gebruik.\n",
    "\n",
    "Meet hoeveel de overlap tussen training, validatie en test-samples.\n",
    "\n",
    "Optionele vragen:\n",
    "- Hoe zit het met bijna duplicaten tussen datasets? (Beelden die bijna identiek zijn)\n",
    "- Maak een schone/clean validatie- en testset en vergelijk de nauwkeurigheid met die in de daaropvolgende opdrachten.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8oww1s4JMQx"
   },
   "source": [
    "---\n",
    "Probleem/vraag 6\n",
    "---------\n",
    "Laten we een idee te krijgen van wat een `on-the-shelf` classificator kan opleveren met deze gegevens. Het is altijd goed om te checken of er iets is om te leren en dat het een probleem is dat niet zo triviaal is er al een kant-en-klare oplossing het oplosbaar maakt.\n",
    "\n",
    "Train een simpel model op deze gegevens met behulp van 50, 100, 1000 en 5000 trainings samples. \n",
    "\n",
    "Hint: u kunt het LogisticRegression model van sklearn.linear_model gebruiken.\n",
    "\n",
    "Optionele vraag: train een `on-the-shelf model` op alle gegevens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
